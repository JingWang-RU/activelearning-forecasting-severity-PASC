{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "44e91fc8-bd55-4f3a-8ef4-1a523bfa7a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ac776d-d5ea-4e88-976f-bcdd768fefc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Define MLP classifier with sparse and mean mapping layers\n",
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=126):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        self.attention = nn.Linear(input_dim, input_dim)  # Per-feature attention\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_weights = torch.sigmoid(self.attention(x))  # Compute per-feature attention\n",
    "        feature_importance = x * attn_weights  # Multiply feature values by attention scores\n",
    "        x = self.activation(self.fc1(feature_importance))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.softmax(self.output_layer(x))\n",
    "        return x, feature_importance  # Return both predictions and feature importance\n",
    "\n",
    "def pad_or_truncate_embeddings(grouped_list, max_len, feature_dim):\n",
    "    total_events = len(grouped_list)\n",
    "    if total_events >= max_len:\n",
    "        return np.array(grouped_list[:max_len]).flatten()\n",
    "    else:\n",
    "        pad = np.zeros((max_len - total_events, feature_dim))\n",
    "        return np.vstack([grouped_list, pad]).flatten()\n",
    "\n",
    "def pad_or_truncate_time(grouped_list, max_len, feature_dim=1):\n",
    "    total_events = len(grouped_list)\n",
    "    if total_events >= max_len:\n",
    "        return np.array(grouped_list[:max_len]).flatten()\n",
    "    else:\n",
    "        pad = np.zeros((max_len - total_events, feature_dim))\n",
    "        return np.vstack([grouped_list, pad]).flatten()\n",
    "\n",
    "class EmbeddingReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=32):\n",
    "        super(EmbeddingReducer, self).__init__()\n",
    "        self.projection = nn.Linear(input_dim, output_dim)  # Linear layer for dimension reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "# Function to get reduced embeddings\n",
    "def get_event_embeddings(events):\n",
    "    embeddings = model.encode(events)  # (batch_size, 768)\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "    reduced_embeddings = reducer(embeddings)  # (batch_size, 32)\n",
    "    return reduced_embeddings\n",
    "    \n",
    "def get_event_embeddings_bert(events):\n",
    "    # tokenized_events = tokenizer(events, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     embeddings = transformer_model(**tokenized_events).pooler_output.cpu().numpy()\n",
    "    # return embeddings\n",
    "\n",
    "    tokenized_events = tokenizer(events, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        decoder_outputs = bio_model.encoder(tokenized_events, encoder_hidden_states=encoder_outputs.last_hidden_state)\n",
    "        embeddings = decoder_outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embeddings\n",
    "def check_nan_inf(tensor, name=\"Tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"{name} contains NaN values!\")\n",
    "        return True\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"{name} contains Inf values!\")\n",
    "        return True\n",
    "\n",
    "def to_tensor(arr): return torch.tensor(np.array(arr), dtype=torch.float32).to(device)\n",
    "def detect_feature_importance(iteration_folder, iteration, flag, filenames_train, feature_importance, total_feature_dim, event_mapping, num_subiter, top_k=5):\n",
    "    \"\"\"\n",
    "    Save important features for all samples and all sub-iterations.\n",
    "\n",
    "    Args:\n",
    "        iteration_folder: Directory to save the files.\n",
    "        iteration: The current iteration index.\n",
    "        flag: 0 for uncertainty sampling, 1 for random sampling.\n",
    "        filenames_train: List of filenames corresponding to the training samples.\n",
    "        feature_importance: Numpy array of feature importance values.\n",
    "        total_feature_dim: Total dimension of each feature vector.\n",
    "        event_mapping: Dictionary mapping filenames to event names.\n",
    "        num_subiter: Number of sub-iterations to process.\n",
    "        top_k: Number of top important features to save (default is 5).\n",
    "    \"\"\"\n",
    "    all_feature_data = []  # Collecting all important features\n",
    "\n",
    "    for i_subiter in range(num_subiter):\n",
    "        # Identify top-k features per sample\n",
    "        top_features_per_sample = np.argsort(-feature_importance, axis=1)[:, :top_k]\n",
    "        \n",
    "        # Extract important features for each training example\n",
    "        feature_importance_dict = {}\n",
    "        for i_train in range(len(filenames_train)):\n",
    "            filename = filenames_train[i_train]\n",
    "            feature_importance_dict[filename] = []\n",
    "\n",
    "            for idx_train in top_features_per_sample[i_train]:\n",
    "                event_idx = idx_train // total_feature_dim\n",
    "                feature_pos = idx_train % total_feature_dim\n",
    "                \n",
    "                # Get event name\n",
    "                if feature_pos < 32:\n",
    "                    event_name = event_mapping.get(filename, [\"Unknown\"])[event_idx] if event_idx < len(event_mapping.get(filename, [])) else f\"Unknown Event {event_idx}\"\n",
    "                else:\n",
    "                    event_name = \"Time\"\n",
    "                \n",
    "                # Record the feature importance score\n",
    "                feature_importance_score = feature_importance[i_train, idx_train]\n",
    "                feature_importance_dict[filename].append((event_name, feature_importance_score))\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        for filename, feature_list in feature_importance_dict.items():\n",
    "            for event, importance in feature_list:\n",
    "                all_feature_data.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"event_name\": event,\n",
    "                    \"importance\": importance,\n",
    "                    \"sub_iteration\": i_subiter\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame and save\n",
    "    df_feature_importance = pd.DataFrame(all_feature_data)\n",
    "    return df_feature_importance\n",
    "\n",
    "print(\"finish loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a64d628-004a-42fb-aeff-fdc328a67a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee309d5-9620-4dd0-b5f8-8d1de699ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load feature data\n",
    "lc_files = [\"PMC10008181\", \"PMC10077184\", \"PMC10129030\", \"PMC10173208\", \"PMC10284064\", \"PMC10469423\", \"PMC10476922\", \"PMC8132077\", \"PMC8511908\", \"PMC8606980\", \"PMC8850995\", \"PMC8958594\", \"PMC9066079\", \"PMC9451509\", \"PMC9514285\", \"PMC9633038\", \"PMC8405236\",\"PMC9451509\"]\n",
    "des_path = \"/data/wangj47/script/annote/activelearning/am_18_llm\"\n",
    "data_list = []\n",
    "for lc_file in lc_files:\n",
    "    file_path = os.path.join(des_path, lc_file + '.txt')\n",
    "    df_temp = pd.read_csv(file_path, sep='\\t', names=['event', 'time'], dtype={'event': str, 'time': float})\n",
    "    df_temp['filename'] = lc_file\n",
    "    data_list.append(df_temp)\n",
    "df_features = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Load label data\n",
    "am_path = \"/data/wangj47/script/annote/activelearning/\"\n",
    "df_labels = pd.read_csv(am_path+\"am_risk_annote_18.csv\")\n",
    "print(\"load data\")\n",
    "\n",
    "# Initialize the reduction layer\n",
    "reducer = EmbeddingReducer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f486b138-ded6-4751-adbd-dca6e92b4f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature generated\n",
      "merged \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentenceTransformer(\"neuml/pubmedbert-base-embeddings\").to(device)\n",
    "# Define a Linear Projection Layer\n",
    "# Generate event embeddings\n",
    "df_features[\"embedding\"] = list(get_event_embeddings(df_features[\"event\"].tolist()).detach().numpy())\n",
    "# Normalize time feature\n",
    "scaler = StandardScaler()\n",
    "df_features[\"time_normalized\"] = scaler.fit_transform(df_features[[\"time\"]])\n",
    "df_features['time_normalized'] = [[x] for x in df_features['time_normalized']]\n",
    "print(\"feature generated\")\n",
    "\n",
    "# Set maximum number of event/time pairs per filename\n",
    "MAX_EVENTS = 150\n",
    "FEATURE_DIM = 32\n",
    "\n",
    "# Aggregate features by filename\n",
    "df_grouped = df_features.groupby(\"filename\").agg({\n",
    "    \"embedding\": lambda x: pad_or_truncate_embeddings(list(x), MAX_EVENTS, FEATURE_DIM),\n",
    "    \"time_normalized\": lambda x: pad_or_truncate_time(list(x), MAX_EVENTS, 1)\n",
    "}).reset_index()\n",
    "\n",
    "df_merged = df_grouped.merge(df_labels, on=\"filename\", how=\"left\").fillna(0)\n",
    "X = df_merged.apply(lambda row: np.hstack([row['embedding'], row['time_normalized']]), axis=1)\n",
    "# Merge labels\n",
    "y = df_merged[\"risk\"].values\n",
    "print(\"merged \")\n",
    "\n",
    "filenames = df_merged[\"filename\"].values  # Store filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89dde119-58cc-4bb8-b394-e50a3442de3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_features.head()#\n",
    "df_features.loc[0]['embedding'].shape#768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d4d9636-64f8-41bc-91af-240eeae53f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "total_feature_dim = 33  # 32 for event embedding + 1 for time\n",
    "\n",
    "# Create base directory for saving results\n",
    "base_dir = \"/data/wangj47/script/annote/activelearning/active_learning_results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "event_mapping = {}\n",
    "for filename in df_features[\"filename\"].unique():\n",
    "    events = df_features[df_features[\"filename\"] == filename][\"event\"].tolist()\n",
    "    event_mapping[filename] = events  # Store ordered event names for each filename\n",
    "\n",
    "index = 0\n",
    "top_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1a9c37a-de54-42df-9f7a-9968b769252a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5fac20fd-a4bc-4582-87de-c73f46286c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6560cd60-20b2-4937-b05a-67c7e10c58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_train = 2\n",
    "model = AttentionMLP(input_dim=len(X_train[0])).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Train the model before looping over flags\n",
    "for epoch in range(n_train):  \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _ = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the trained model's state_dict\n",
    "initial_model_state = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cffba49-e120-486a-bdbf-bec7d1ebaa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterat:  0\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_1/feature_importance_test_0_flag_0_iter_0_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_1/feature_importance_test_0_flag_1_iter_0_subiter_7.csv\n",
      "iterat:  1\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_2/feature_importance_test_0_flag_0_iter_1_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_2/feature_importance_test_0_flag_1_iter_1_subiter_7.csv\n",
      "iterat:  2\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_3/feature_importance_test_0_flag_0_iter_2_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_3/feature_importance_test_0_flag_1_iter_2_subiter_7.csv\n",
      "iterat:  3\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_4/feature_importance_test_0_flag_0_iter_3_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_4/feature_importance_test_0_flag_1_iter_3_subiter_7.csv\n",
      "iterat:  4\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_5/feature_importance_test_0_flag_0_iter_4_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_5/feature_importance_test_0_flag_1_iter_4_subiter_7.csv\n",
      "iterat:  5\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_6/feature_importance_test_0_flag_0_iter_5_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_6/feature_importance_test_0_flag_1_iter_5_subiter_7.csv\n",
      "iterat:  6\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_7/feature_importance_test_0_flag_0_iter_6_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_7/feature_importance_test_0_flag_1_iter_6_subiter_7.csv\n",
      "iterat:  7\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_8/feature_importance_test_0_flag_0_iter_7_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_8/feature_importance_test_0_flag_1_iter_7_subiter_7.csv\n",
      "iterat:  8\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_9/feature_importance_test_0_flag_0_iter_8_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_9/feature_importance_test_0_flag_1_iter_8_subiter_7.csv\n",
      "iterat:  9\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_10/feature_importance_test_0_flag_0_iter_9_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_10/feature_importance_test_0_flag_1_iter_9_subiter_7.csv\n",
      "iterat:  10\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_11/feature_importance_test_0_flag_0_iter_10_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_11/feature_importance_test_0_flag_1_iter_10_subiter_7.csv\n",
      "iterat:  11\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_12/feature_importance_test_0_flag_0_iter_11_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_12/feature_importance_test_0_flag_1_iter_11_subiter_7.csv\n",
      "iterat:  12\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_13/feature_importance_test_0_flag_0_iter_12_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_13/feature_importance_test_0_flag_1_iter_12_subiter_7.csv\n",
      "iterat:  13\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_14/feature_importance_test_0_flag_0_iter_13_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_14/feature_importance_test_0_flag_1_iter_13_subiter_7.csv\n",
      "iterat:  14\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_15/feature_importance_test_0_flag_0_iter_14_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_15/feature_importance_test_0_flag_1_iter_14_subiter_7.csv\n",
      "iterat:  15\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_16/feature_importance_test_0_flag_0_iter_15_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_16/feature_importance_test_0_flag_1_iter_15_subiter_7.csv\n",
      "iterat:  16\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_17/feature_importance_test_0_flag_0_iter_16_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_17/feature_importance_test_0_flag_1_iter_16_subiter_7.csv\n",
      "iterat:  17\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_18/feature_importance_test_0_flag_0_iter_17_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_18/feature_importance_test_0_flag_1_iter_17_subiter_7.csv\n",
      "iterat:  18\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_19/feature_importance_test_0_flag_0_iter_18_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_19/feature_importance_test_0_flag_1_iter_18_subiter_7.csv\n",
      "iterat:  19\n",
      "Train samples: 3, Test samples: 7, Pool samples: 8\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_20/feature_importance_test_0_flag_0_iter_19_subiter_7.csv\n",
      "feature importance saved as  /data/wangj47/script/annote/activelearning/active_learning_results/test_size_7/iteration_20/feature_importance_test_0_flag_1_iter_19_subiter_7.csv\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 20\n",
    "n_train = 20\n",
    "num_train_test=[[11/18,3/7],[5/9,3/8],[1/2,1/3],[4/9,3/10]]\n",
    "num_test = [4,5,6,7]\n",
    "num_train_test=[[4/9,3/10]]\n",
    "num_test = [7]#,5,6,7]\n",
    "for j, train_test_ratio in enumerate(num_train_test):\n",
    "    test_size_folder = os.path.join(base_dir, f\"test_size_{num_test[j]}\")\n",
    "    os.makedirs(test_size_folder, exist_ok=True)\n",
    "    df_accuracy = pd.DataFrame()\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"iterat: \",iteration)\n",
    "        iteration_folder = os.path.join(test_size_folder, f\"iteration_{iteration+1}\")\n",
    "        os.makedirs(iteration_folder, exist_ok=True)\n",
    "        acc_folder = iteration_folder#os.path.join(test_size_folder, f\"accuracy\")\n",
    "        # os.makedirs(acc_folder, exist_ok=True)\n",
    "\n",
    "        X_initial, X_pool, y_initial, y_pool, filenames_train, filenames_pool = train_test_split(\n",
    "            X, y, filenames, test_size=train_test_ratio[0], random_state=42\n",
    "        )\n",
    "        \n",
    "        X_test, X_train, y_test, y_train, filenames_test, filenames_train = train_test_split(\n",
    "            X_initial, y_initial, filenames_train, test_size=train_test_ratio[1], random_state=42\n",
    "        )\n",
    "        # Convert pandas DataFrames/Series to numpy arrays\n",
    "        X_train = list(X_train.values)\n",
    "        y_train = list(y_train)\n",
    "        X_test = list(X_test.values)\n",
    "        y_test = list(y_test)\n",
    "        X_pool = list(X_pool.values)\n",
    "        # print(\"train \", len(y_train), \"test \", len(y_test), \"pool \", len(X_pool))\n",
    "        feature_shape = len(X_train[0])\n",
    "        original_X_train, original_y_train = X_train.copy(), y_train.copy()\n",
    "        original_X_pool, original_y_pool = X_pool.copy(), y_pool.copy()\n",
    "        original_filenames_train = filenames_train.copy()\n",
    "        original_filenames_pool = filenames_pool.copy()\n",
    "        \n",
    "        X_test_tensor = torch.tensor(np.array(X_test), dtype=torch.float32).to(device)\n",
    "        X_train_tensor = torch.tensor(np.array(X_train), dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(np.array(y_test), dtype=torch.long).to(device)\n",
    "        y_train_tensor = torch.tensor(np.array(y_train), dtype=torch.long).to(device)\n",
    "\n",
    "        # Print information for verification (Optional)\n",
    "        print(f\"Train samples: {len(y_train)}, Test samples: {len(y_test)}, Pool samples: {len(X_pool)}\")\n",
    "\n",
    "        result_al_acc, result_al_index = [],[] # Accuracy results for flag=0 (uncertainty)\n",
    "        result_rd_acc, result_rd_index= [],[]  # Accuracy results for flag=1 (random)\n",
    "        al_selected_sample, rd_selected_sample = [], []\n",
    "        num_subiter = len(X_pool)\n",
    "\n",
    "        # Train the model on the initial training data before the active learning process\n",
    "        model = AttentionMLP(input_dim=len(X_train[0])).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "        \n",
    "        # Prepare the original training tensors\n",
    "        X_train_tensor = torch.tensor(np.array(X_train), dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(np.array(y_train), dtype=torch.long).to(device)\n",
    "        \n",
    "        # Train the model on the original training data before active learning\n",
    "        for epoch in range(n_train):  \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save the trained model's state\n",
    "        initial_model_state = model.state_dict()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                    predictions, attn_weights = model(X_test_tensor)\n",
    "                    accuracy = (predictions.argmax(dim=1) == y_test_tensor).float().mean().item()\n",
    "        result_al_acc.append(accuracy)\n",
    "        result_rd_acc.append(accuracy)\n",
    "        # Active Learning Loop for flag=0 and flag=1\n",
    "        for flag in range(2):  # flag=0: uncertainty sampling, flag=1: random sampling\n",
    "            # Reload the trained model's state for both flags\n",
    "            \n",
    "            X_train = original_X_train.copy()\n",
    "            y_train = original_y_train.copy()\n",
    "            X_pool = original_X_pool.copy()\n",
    "            y_pool = original_y_pool.copy()\n",
    "            filenames_train = original_filenames_train.copy()\n",
    "            filenames_pool = original_filenames_pool.copy()\n",
    "            num_subiter = len(X_pool)\n",
    "            df_features = pd.DataFrame()\n",
    "            \n",
    "            for i in range(num_subiter):\n",
    "                if len(X_pool) > 1:\n",
    "                    with torch.no_grad():\n",
    "                        probs, _ = model(to_tensor(X_pool))\n",
    "                        probs = probs.cpu().numpy()\n",
    "                        uncertainty = np.abs(probs[:, 0] - probs[:, 1])\n",
    "                    \n",
    "                    if flag == 0:  # Uncertainty Sampling\n",
    "                        next_idx = np.argmin(uncertainty)\n",
    "                        result_al_index.append(next_idx)                        \n",
    "                    else:  # Random Sampling\n",
    "                        next_idx = np.random.randint(len(X_pool))\n",
    "                        result_rd_index.append(next_idx)\n",
    "                    \n",
    "                    # Add the selected sample to training set\n",
    "                    X_train = np.vstack([X_train, X_pool[next_idx]])\n",
    "                    y_train = np.append(y_train, y_pool[next_idx])\n",
    "                    filenames_train = np.append(filenames_train, filenames_pool[next_idx])\n",
    "                    \n",
    "                    # Remove from pool\n",
    "                    X_pool = np.delete(np.array(X_pool), next_idx, axis=0)\n",
    "                    y_pool = np.delete(np.array(y_pool), next_idx)\n",
    "                    filenames_pool = np.delete(filenames_pool, next_idx)\n",
    "                \n",
    "                elif len(X_pool) == 1:  # If only one sample left\n",
    "                    if isinstance(X_train, np.ndarray):\n",
    "                        X_train = list(X_train)\n",
    "                        y_train = list(y_train)\n",
    "                    X_train.append(X_pool[0])\n",
    "                    y_train.append(y_pool[0])\n",
    "                    filenames_train = np.append(filenames_train, filenames_pool[0])\n",
    "                    X_train = np.array(X_train)\n",
    "                    y_train = np.array(y_train)\n",
    "                    X_pool, y_pool, filenames_pool = [], [], []\n",
    "                for epoch in range(n_train):  \n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs, _ = model(X_train_tensor)\n",
    "                    loss = criterion(outputs, y_train_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                            predictions, attn_weights = model(X_test_tensor)\n",
    "                            accuracy = (predictions.argmax(dim=1) == y_test_tensor).float().mean().item()\n",
    "                if flag==0: result_al_acc.append(accuracy)\n",
    "                else: result_rd_acc.append(accuracy)\n",
    "                # Identify important features\n",
    "                with torch.no_grad():\n",
    "                    _, attn_weights = model(torch.tensor(np.array(X_train), dtype=torch.float32).to(device))\n",
    "                    feature_importance = attn_weights.cpu().detach().numpy()\n",
    "                top_features_per_sample = np.argsort(-feature_importance, axis=1)[:, :top_k]\n",
    "                featuers_select = detect_feature_importance(iteration_folder, iteration, flag, filenames_train, feature_importance, total_feature_dim, event_mapping, num_subiter, top_k=5)\n",
    "                df_features = pd.concat([df_features, featuers_select], ignore_index=True)\n",
    "\n",
    "            feature_file = os.path.join(iteration_folder, f\"feature_importance_test_{num_test[j]}_flag_{flag}_iter_{iteration}_subiter_{i}.csv\")\n",
    "            df_features.to_csv(feature_file, index=False)\n",
    "            print(\"feature importance saved as \", feature_file)\n",
    "        acc_data = []\n",
    "        acc_data.append({\n",
    "                \"iteration\": iteration,\n",
    "                \"uncertainty_accuracy\": result_al_acc,\n",
    "                \"uncertainty_selected\":result_al_index,\n",
    "                \"random_accuracy\": result_rd_acc,\n",
    "                \"random_selected\": result_rd_index\n",
    "            })\n",
    "        iter_accuracy = pd.DataFrame(acc_data)\n",
    "        df_accuracy = pd.concat([df_accuracy, iter_accuracy], ignore_index=True)\n",
    "\n",
    "    accuracy_file = os.path.join(acc_folder, f\"accuracy_results_split_{j}.csv\")\n",
    "    df_accuracy.to_csv(accuracy_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7124798e-cfaa-48ea-8aac-9d31e4e0114b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_accuracy.iloc[12]['random_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d050087-143b-4566-a595-b834f6729e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "17dcdb73-935a-4a05-8eef-c8061b95ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53ef6a0-9218-45cf-b8ea-5785c2d6fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_acc['uncertainty_accuracy']\n",
    "# df_acc['random_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e9adc4bc-f55b-4eee-8065-8910a2756ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.5714285969734192, 0.7142857313156128, 0.7142857313156128, 0.7142857313156128, 0.4285714626312256, 0.2857142984867096, 0.2857142984867096, 0.2857142984867096]'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [0,6,9,12,18]\n",
    "df_acc.iloc[index[0]]['uncertainty_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34047e3-b73f-4a29-807d-99151a822c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a307646f-203f-45e4-8a72-bc1130e2d3c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: [0.5714285969734192, 0.7142857313156128, 0.7142857313156128, 0.7142857313156128, 0.4285714626312256, 0.2857142984867096, 0.2857142984867096, 0.2857142984867096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_acc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muncertainty_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_acc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muncertainty_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_acc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_acc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(ast\u001b[38;5;241m.\u001b[39mliteral_eval)\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/ast.py:99\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/ast.py:98\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/ast.py:75\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/ast.py:66\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/gpfs/gsfs12/users/wangj47/conda/envs/nltk_envs/lib/python3.8/ast.py:63\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_malformed_node\u001b[39m(node):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmalformed node or string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string: [0.5714285969734192, 0.7142857313156128, 0.7142857313156128, 0.7142857313156128, 0.4285714626312256, 0.2857142984867096, 0.2857142984867096, 0.2857142984867096]"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# df_acc['uncertainty_accuracy'] = df_acc['uncertainty_accuracy'].apply(ast.literal_eval)\n",
    "df_acc['random_accuracy'] = df_acc['random_accuracy'].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b3741-c8ad-4252-8bca-5fb1737e9701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb369eb-9fbc-4a39-8695-369256528a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a6393-3c01-4afb-8169-bea3f18e5e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872bdf9-10a5-48bf-993e-60fc9c6d5349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nltk)",
   "language": "python",
   "name": "nltk_envs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
